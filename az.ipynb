{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepgroebner.wrapped import CLeadMonomialsEnv as LeadMonomialsEnv\n",
    "from deepgroebner.networks import ParallelMultilayerPerceptron, ParallelEmbeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentValueModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super(RecurrentValueModel, self).__init__()\n",
    "        self.embedding = ParallelEmbeddingLayer(units, [])\n",
    "        self.rnn = tf.keras.layers.LSTM(units)\n",
    "        self.dense = tf.keras.layers.Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, batch):\n",
    "        return self.dense(self.rnn(self.embedding(batch)))\n",
    "\n",
    "\n",
    "class GlobalSumPooling1D(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GlobalSumPooling1D, self).__init__()\n",
    "\n",
    "    def call(self, batch, mask=None):\n",
    "        if mask is not None:\n",
    "            batch = batch * tf.cast(tf.expand_dims(mask, -1), tf.float32)\n",
    "        return tf.reduce_sum(batch, axis=-2)\n",
    "\n",
    "\n",
    "class PoolingValueModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, hidden_layers1, hidden_layers2, method='max'):\n",
    "        super(PoolingValueModel, self).__init__()\n",
    "        self.embedding = ParallelEmbeddingLayer(hidden_layers1[-1], hidden_layers1[:-1])\n",
    "        if method == 'max':\n",
    "            self.pooling = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        elif method == 'mean':\n",
    "            self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        elif method == 'sum':\n",
    "            self.pooling = GlobalSumPooling1D()\n",
    "        else:\n",
    "            raise ValueError('invalid method')\n",
    "        self.hidden_layers = [tf.keras.layers.Dense(u, activation='relu') for u in hidden_layers2]\n",
    "        self.final_layer = tf.keras.layers.Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, batch):\n",
    "        X = self.pooling(self.embedding(batch))\n",
    "        for layer in self.hidden_layers:\n",
    "            X = layer(X)\n",
    "        return self.final_layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AZWrapper:\n",
    "    \"\"\"A wrapper for LeadMonomialsEnv environments to interact with the AZAgent.\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.players = 1\n",
    "        self.turn = 0\n",
    "        self.state = None\n",
    "        self.done = None\n",
    "        self.actions = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.env.reset()\n",
    "        self.done = False\n",
    "        self.actions = list(range(len(self.state)))\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state, reward, self.done, info = self.env.step(action)\n",
    "        self.actions = list(range(len(self.state)))\n",
    "        return self.state, reward, self.done, info\n",
    "\n",
    "    def copy(self):\n",
    "        copy = AZWrapper(self.env.copy())\n",
    "        copy.state = self.state.copy()\n",
    "        copy.done = self.done\n",
    "        copy.actions = self.actions.copy()\n",
    "        return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AZTreeNode:\n",
    "    \"\"\"A tree node for AlphaZero tree search.\"\"\"\n",
    "\n",
    "    def __init__(self, parent, action, reward, env, logpi, value):\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.env = env\n",
    "        self.visits = 0\n",
    "        self.logpi = logpi\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "def az_ucb(c=np.sqrt(2)):\n",
    "    \"\"\"Return an upper confidence bound tree policy for AlphaZero tree search.\"\"\"\n",
    "    def policy(node):\n",
    "        def value(child):\n",
    "            prob = np.exp(node.logpi[child.action])\n",
    "            return child.value[child.env.turn] + c * prob * np.sqrt(node.visits)/(1 + child.visits)\n",
    "        return max(node.children, key=value)\n",
    "    return policy\n",
    "\n",
    "\n",
    "class AZBuffer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.values = []\n",
    "        \n",
    "    def store(self, state, prob, value):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(prob)\n",
    "        self.values.append(value)\n",
    "        \n",
    "    def clear(self):\n",
    "        self.states.clear()\n",
    "        self.probs.clear()\n",
    "        self.values.clear()\n",
    "    \n",
    "    def get(self, batch_size=32, drop_remainder=False):\n",
    "        \"\"\"Return tf.data.Dataset of training data.\"\"\"\n",
    "        if self.states and self.states[0].ndim == 2:\n",
    "\n",
    "            # filter out any states with only one action available\n",
    "            indices = [i for i in range(len(self.states)) if self.states[i].shape[0] != 1]\n",
    "            states = [self.states[i].astype(np.int32) for i in indices]\n",
    "            probs = [self.probs[i].astype(np.float32) for i in indices]\n",
    "            values = np.array([self.values[i] for i in indices], dtype=np.float32)\n",
    "\n",
    "            dataset = tf.data.Dataset.zip((\n",
    "                tf.data.Dataset.from_generator(lambda: states, tf.int32),\n",
    "                tf.data.Dataset.from_generator(lambda: probs, tf.float32),\n",
    "                tf.data.Dataset.from_tensor_slices(values),\n",
    "            ))\n",
    "            if batch_size is None:\n",
    "                batch_size = len(states)\n",
    "            padded_shapes = ([None, self.states[0].shape[1]], [None,], [None,])\n",
    "            padding_values = (tf.constant(-1, dtype=tf.int32),\n",
    "                              tf.constant(0.0, dtype=tf.float32),\n",
    "                              tf.constant(0.0, dtype=tf.float32))\n",
    "            dataset = dataset.padded_batch(batch_size,\n",
    "                                           padded_shapes=padded_shapes,\n",
    "                                           padding_values=padding_values,\n",
    "                                           drop_remainder=drop_remainder)\n",
    "            return dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "\n",
    "class AZAgent:\n",
    "    \"\"\"An AlphaZero tree search agent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy_network : network\n",
    "        The network that maps states to policies.\n",
    "    value_network : network\n",
    "        The network that maps states to values.\n",
    "    tree_policy : function\n",
    "        A function which maps node to child node.\n",
    "    timeout : float, optional\n",
    "        The amount of time in seconds to search before choosing an action.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_network, value_network, tree_policy=az_ucb(), timeout=1.0,\n",
    "                 policy_lr=1e-3, policy_updates=40, value_lr=1e-3, value_updates=40):\n",
    "        self.tree_policy = tree_policy\n",
    "        self.timeout = timeout\n",
    "\n",
    "        self.policy_model = policy_network\n",
    "        self.policy_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(lr=policy_lr)\n",
    "        self.policy_updates = policy_updates\n",
    "\n",
    "        self.value_model = value_network\n",
    "        self.value_loss = tf.keras.losses.MSE\n",
    "        self.value_optimizer = tf.keras.optimizers.Adam(lr=value_lr)\n",
    "        self.value_updates = value_updates\n",
    "\n",
    "    def act(self, env, root=None):\n",
    "        \"\"\"Return a chosen action for the env.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : environment\n",
    "            The current environment.\n",
    "        root : AZTreeNode\n",
    "            The root of the tree corresponding to env state if a subtree has already been explored.\n",
    "\n",
    "        \"\"\"\n",
    "        if root is None:\n",
    "            root = AZTreeNode(None, None, 0.0, env.copy(),\n",
    "                              self.policy(env.state),\n",
    "                              self.value(env.state))\n",
    "        limit = time.time() + self.timeout\n",
    "        counter =0\n",
    "        while time.time() < limit:\n",
    "            leaf = self.expand(root)\n",
    "            value = 0.0 if leaf.env.done else self.value(leaf.env.state)\n",
    "            self.backup(leaf, value)\n",
    "            counter += 1\n",
    "#         print(counter)\n",
    "        return max(root.children, key=lambda node: node.visits).action\n",
    "\n",
    "    @tf.function\n",
    "    def policy(self, state):\n",
    "        \"\"\"Return the predicted policy for the given state using the policy model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.array\n",
    "            The state of the environment.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.policy_model(state[tf.newaxis])[0]\n",
    "\n",
    "    @tf.function\n",
    "    def value(self, state):\n",
    "        \"\"\"Return the predicted value for the given state using the value model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.array\n",
    "            The state of the environment.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.value_model(state[tf.newaxis])[0]\n",
    "\n",
    "    def run_episode(self, env, buffer=None):\n",
    "        env.reset()\n",
    "        root = AZTreeNode(None, None, 0.0, env.copy(),\n",
    "                          self.policy(env.state),\n",
    "                          self.value(env.state))\n",
    "        total_reward = 0.0\n",
    "        length = 0\n",
    "        while not env.done:\n",
    "            action = self.act(env, root=root)\n",
    "#             print(\"Just took an action: \" +str(action))\n",
    "            if buffer is not None:\n",
    "                visits = np.array([child.visits for child in root.children])\n",
    "                probs = (visits / np.sum(visits)).astype(np.float32)\n",
    "                buffer.store(root.env.state, probs, root.value)\n",
    "            _, reward, _, _ = env.step(action)\n",
    "            root = root.children[action]\n",
    "            root.parent = None\n",
    "            total_reward += reward\n",
    "            length += 1\n",
    "#             print(\"Env state: \\n\")\n",
    "#             print(env.state)\n",
    "#             print(\"Root state: \\n\")\n",
    "#             print(root.env.state)\n",
    "#             print(\"Probs: \\n\")\n",
    "#             print(probs)\n",
    "#             print(\"Root val: \\n\")\n",
    "#             print(root.value)\n",
    "#             print(\"Root visits: \\n\")\n",
    "#             print(root.visits)\n",
    "        print(env.chosen)\n",
    "        print(\"End of ep\")\n",
    "        return total_reward, length\n",
    "\n",
    "    def run_episodes(self, env, episodes=100, buffer=None):\n",
    "        history = {'returns': np.zeros(episodes),\n",
    "                   'lengths': np.zeros(episodes)}\n",
    "        for i in range(episodes):\n",
    "            R, L = self.run_episode(env, buffer=buffer)\n",
    "            history['returns'][i] = R\n",
    "            history['lengths'][i] = L\n",
    "        return history\n",
    "\n",
    "    def train(self, env, episodes=100, epochs=1):\n",
    "        buffer = AZBuffer()\n",
    "        history = {'mean_returns': np.zeros(epochs)}\n",
    "        for epoch in range(epochs):\n",
    "            run_history = self.run_episodes(env, episodes=episodes, buffer=buffer)\n",
    "            dataset = buffer.get()\n",
    "            self._fit_policy_model(dataset, epochs=self.policy_updates)\n",
    "            self._fit_value_model(dataset, epochs=self.value_updates)\n",
    "            history['mean_returns'] = np.mean(run_history['returns'])\n",
    "            buffer.clear()\n",
    "        return history\n",
    "\n",
    "    def expand(self, node):\n",
    "        \"\"\"Return an unvisited or terminal leaf node following the tree policy.\n",
    "\n",
    "        Before returning, this function performs all possible actions from the\n",
    "        leaf node and adds new nodes for them to the tree as children of the\n",
    "        leaf node.\n",
    "        \"\"\"\n",
    "        while node.visits != 0 and len(node.children) > 0:\n",
    "            node = self.tree_policy(node)\n",
    "        if not node.env.done:\n",
    "            for action in node.env.actions:\n",
    "                env = node.env.copy()\n",
    "                _, reward, _, _ = env.step(action)\n",
    "                node.children.append(AZTreeNode(node, action, reward, env,\n",
    "                                                self.policy(env.state),\n",
    "                                                self.value(env.state)))\n",
    "        return node\n",
    "\n",
    "    def backup(self, node, value):\n",
    "        \"\"\"Backup the value from a new leaf node.\"\"\"\n",
    "        while node is not None:\n",
    "            value += node.reward\n",
    "            node.visits += 1\n",
    "            node.value = (node.visits - 1)/node.visits * node.value + value/node.visits\n",
    "            node = node.parent\n",
    "\n",
    "    def _fit_policy_model(self, dataset, epochs=1):\n",
    "        \"\"\"Fit value model using data from dataset.\"\"\"\n",
    "        history = {'loss': []}\n",
    "        for epoch in range(epochs):\n",
    "            loss, batches = 0, 0\n",
    "            for states, probs, _ in dataset:\n",
    "                batch_loss = self._fit_policy_model_step(states, probs)\n",
    "                loss += batch_loss\n",
    "                batches += 1\n",
    "            history['loss'].append(loss / batches)\n",
    "        return {k: np.array(v) for k, v in history.items()}\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def _fit_policy_model_step(self, states, probs):\n",
    "        \"\"\"Fit value model on one batch of data.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            logpis = self.policy_model(states, training=True)\n",
    "            loss = tf.reduce_mean(self.policy_loss(probs, logpis))\n",
    "        varis = self.policy_model.trainable_variables\n",
    "        grads = tape.gradient(loss, varis)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads, varis))\n",
    "        return loss\n",
    "    \n",
    "    def _fit_value_model(self, dataset, epochs=1):\n",
    "        \"\"\"Fit value model using data from dataset.\"\"\"\n",
    "        history = {'loss': []}\n",
    "        for epoch in range(epochs):\n",
    "            loss, batches = 0, 0\n",
    "            for states, _, values in dataset:\n",
    "                batch_loss = self._fit_value_model_step(states, values)\n",
    "                loss += batch_loss\n",
    "                batches += 1\n",
    "            history['loss'].append(loss / batches)\n",
    "        return {k: np.array(v) for k, v in history.items()}\n",
    "\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def _fit_value_model_step(self, states, values):\n",
    "        \"\"\"Fit value model on one batch of data.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_values = self.value_model(states, training=True)\n",
    "            loss = tf.reduce_mean(self.value_loss(values, pred_values))\n",
    "        varis = self.value_model.trainable_variables\n",
    "        grads = tape.gradient(loss, varis)\n",
    "        self.value_optimizer.apply_gradients(zip(grads, varis))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AZWrapper(LeadMonomialsEnv('3-20-10-weighted', k=2))\n",
    "policy_network = ParallelMultilayerPerceptron([128])\n",
    "value_network = PoolingValueModel([128], [128], method='sum')\n",
    "agent = AZAgent(policy_network, value_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = agent.train(env, episodes=3, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_returns': -128.33333333333334}\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toyenvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = toyenvs.DumbTicTacToeEnv(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation=tf.nn.log_softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = AZBuffer()\n",
    "agent2 = AZAgent(policy_net, val_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent3 = AZAgent(policy_net, val_net, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 1, 3: 1, 1: 1, 0: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 3: 1, 1: 1, 0: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 3: 1, 1: 1, 0: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n",
      "{2: 1, 0: 1, 3: 1, 1: 1}\n",
      "End of ep\n"
     ]
    }
   ],
   "source": [
    "run_history = agent3.run_episodes(ttt, episodes=50, buffer=buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'returns': array([-4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4.,\n",
      "       -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4.,\n",
      "       -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4.,\n",
      "       -4., -4., -4., -4., -4., -4., -4., -4., -4., -4., -4.]), 'lengths': array([4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n",
      "       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.])}\n"
     ]
    }
   ],
   "source": [
    "print(run_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
